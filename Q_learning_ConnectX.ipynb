{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "47XsPVDf0jV9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class connectX :\n",
        "  def __init__(self , rows=6,columns=7,connect = 4):\n",
        "    self.rows = rows\n",
        "    self.columns = columns\n",
        "    self.connect = connect\n",
        "    self.board = np.zeros((rows,columns),dtype=int)\n",
        "    self.current_player = 1\n",
        "\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((self.rows,self.columns),dtype=int)\n",
        "    self.current_player = 1\n",
        "    return self.board\n",
        "\n",
        "  def dropdisc(self,column):\n",
        "    for row in range(self.rows-1,-1,-1):\n",
        "      if self.board[row][column] == 0:\n",
        "        self.board[row][column] = self.current_player\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def is_winning_move(self, player):\n",
        "        # Check horizontal, vertical, and diagonal combinations\n",
        "        # Horizontal\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.columns - self.connect + 1):\n",
        "                if all(self.board[row, col + i] == player for i in range(self.connect)):\n",
        "                    return True\n",
        "\n",
        "        # Vertical\n",
        "        for col in range(self.columns):\n",
        "            for row in range(self.rows - self.connect + 1):\n",
        "                if all(self.board[row + i, col] == player for i in range(self.connect)):\n",
        "                    return True\n",
        "\n",
        "        # Diagonal \\\n",
        "        for row in range(self.rows - self.connect + 1):\n",
        "            for col in range(self.columns - self.connect + 1):\n",
        "                if all(self.board[row + i, col + i] == player for i in range(self.connect)):\n",
        "                    return True\n",
        "\n",
        "        # Diagonal /\n",
        "        for row in range(self.connect - 1, self.rows):\n",
        "            for col in range(self.columns - self.connect + 1):\n",
        "                if all(self.board[row - i, col + i] == player for i in range(self.connect)):\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "  def is_draw(self):\n",
        "        return not np.any(self.board == 0)\n",
        "\n",
        "  def switch_player(self):\n",
        "        self.current_player = 3 - self.current_player  # Switch between 1 and 2\n",
        "\n",
        "  def get_valid_moves(self):\n",
        "        return [col for col in range(self.columns) if self.board[0, col] == 0]\n",
        "\n",
        "  def render(self):\n",
        "        print(np.flip(self.board, 0))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = defaultdict(lambda: np.zeros(7))  # State-action values\n",
        "\n",
        "    def choose_action(self, state, valid_moves):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Exploration\n",
        "            return random.choice(valid_moves)\n",
        "        else:\n",
        "            # Exploitation\n",
        "            q_values = self.q_table[state]\n",
        "            q_values = np.array([q_values[a] if a in valid_moves else -np.inf for a in range(7)])\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, done):\n",
        "        q_current = self.q_table[state][action]\n",
        "        if done:\n",
        "            q_target = reward\n",
        "        else:\n",
        "            q_target = reward + self.gamma * np.max(self.q_table[next_state])\n",
        "\n",
        "        self.q_table[state][action] += self.alpha * (q_target - q_current)\n",
        "\n",
        "    def get_state(self, board):\n",
        "        return tuple(map(tuple, board))\n",
        "\n"
      ],
      "metadata": {
        "id": "am_abD9I13lY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(episodes=10000):\n",
        "    env = connectX()\n",
        "    agent = QLearningAgent()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_repr = agent.get_state(state)\n",
        "            valid_moves = env.get_valid_moves()\n",
        "            action = agent.choose_action(state_repr, valid_moves)\n",
        "            env.dropdisc(action)\n",
        "\n",
        "            if env.is_winning_move(env.current_player):\n",
        "                reward = 1\n",
        "                done = True\n",
        "            elif env.is_draw():\n",
        "                reward = 0\n",
        "                done = True\n",
        "            else:\n",
        "                reward = 0\n",
        "                env.switch_player()\n",
        "\n",
        "            next_state = state\n",
        "            next_state_repr = agent.get_state(next_state)\n",
        "\n",
        "            agent.update_q_value(state_repr, action, reward, next_state_repr, done)\n",
        "\n",
        "            if not done:\n",
        "                env.switch_player()\n",
        "                state = next_state\n",
        "\n",
        "        if (episode + 1) % 1000 == 0:\n",
        "            print(f\"Episode {episode + 1}/{episodes}\")\n",
        "\n",
        "train_agent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG_feuzp17yX",
        "outputId": "db5e8a2e-80bb-4189-99f7-7cd4108f415e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000/10000\n",
            "Episode 2000/10000\n",
            "Episode 3000/10000\n",
            "Episode 4000/10000\n",
            "Episode 5000/10000\n",
            "Episode 6000/10000\n",
            "Episode 7000/10000\n",
            "Episode 8000/10000\n",
            "Episode 9000/10000\n",
            "Episode 10000/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(episodes=1000):\n",
        "    env = connectX()\n",
        "    agent = QLearningAgent()\n",
        "    agent.epsilon = 0  # Set exploration to zero for evaluation\n",
        "    wins = 0\n",
        "    draws = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_repr = agent.get_state(state)\n",
        "            valid_moves = env.get_valid_moves()\n",
        "            action = agent.choose_action(state_repr, valid_moves)\n",
        "            env.dropdisc(action)\n",
        "\n",
        "            if env.is_winning_move(env.current_player):\n",
        "                if env.current_player == 1:\n",
        "                    wins += 1\n",
        "                done = True\n",
        "            elif env.is_draw():\n",
        "                draws += 1\n",
        "                done = True\n",
        "            else:\n",
        "                env.switch_player()\n",
        "\n",
        "    print(f\"Wins: {wins}, Draws: {draws}, Losses: {episodes - wins - draws}\")\n",
        "\n",
        "evaluate_agent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx_ECbtU2qvK",
        "outputId": "ef9c66b1-b601-4602-bb8a-dc3a4a94e998"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wins: 1000, Draws: 0, Losses: 0\n"
          ]
        }
      ]
    }
  ]
}